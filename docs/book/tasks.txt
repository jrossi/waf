
== Task processing

This chapter provides a description of the task classes which are used during the build phase.

=== Task execution

==== Build overview

groups -> producer-consumer -> task states

Creating all tasks by hand is a tedious process that the task generators may automate. Before starting the build, Waf asks each task generator to produce the corresponding tasks. If Waf is launched from a sub folder inside the source directory, it will try to avoid the creation of the tasks that are not relevant for that particular sub folder (optimization).

Here are a few motives for using the task generators to create the tasks instead of creating them manually:

. Lazy creation: building only a part of the project is a common scenario
. Filesystem access: the filesystem requirements such as new folders will be created ahead of time
. Concurrency issues: tasks are executed in parallel and may lead to complex bugs if initialized improperly

A summary can be found on the following diagram:

image::task_execution{PIC}["Flowchart model"{backend@docbook:,width=370:},align="center"]


==== Producer-consumer system

Tasks are executed in parallel by default to take advantage of the hardware (multi-core) or the environment (distributed builds). By default Waf does not execute immediately the tasks that are ready. Instead, tasks are added to a queue which is consumed by threads. Waf detects the number of installed processors. For uni-processor only one task is executed at a time, for dual-processors two tasks are executed at a time, and so on. To disable task parallelization, use the option '-j1'. To enhance parallelization, use the option '-j' with the amount of consumers:

[source,shishell]
------------------
$ waf -j3
------------------

By default, Waf does not allow consumer threads to access the tasks directly:

. There is little need for parallelizing the computation of the next task to execute, choosing the next task is fast enough
. The thread issues are limited to a very small section of the code
. The producer-consumer scheme prevents 'busy waiting' for the next task
. A simple global error handler can be used for processing the errors and to decide to stop the build

The following illustrates the relationship producer-consumer performed for the builds:

image::prodcons{PIC}["Parallel execution"{backend@docbook:,width=500:},align="center"]

==== Task states

Executing a task consists in calling the method 'run' on that task, and setting the task execution state.
The following diagram is a summary of the process:

image::task_run{PIC}["Task execution"{backend@docbook:,width=250:},align="center"]

The method 'post_run' can be used to check if the files have been produced, it must throw an OSError if the task has not completed properly.

==== Build groups

Tasks and task generators are added to build groups. The build groups are then processed one by one during the build phase.

[source,python]
---------------
def build(bld):
	ctx(features='c cprogram', source='main.c', target='mycompiler')
	ctx.add_group()
	ctx(features='c cprogram', source='user.c', target='someotherapp')
---------------

The effect of task groups when running tasks in parallel is illustrated by the following diagram. Three groups of tasks have been added, and the execution of the next group only starts when the execution of the tasks in the previous group is complete.

image::output-ADDGROUP{PIC}["Task groups"{backend@docbook:,width=350:},align="center"]

It is possible to create groups at any point in the scripts, and to add the task generators to any group previously created. Adding groups for specific folders or scripts enables a behaviour similar to projects organized in recursive Makefiles.

[source,python]
---------------
def build(bld):

	bld.add_group('test1')
	bld.add_group('test2')
	bld.add_group('test3')
	bld.add_group('test4')

	print('adding task generators')

	bld.set_group('test3')
	bld(features='cxx cprogram', source='main3.c', target='g3')

	bld.set_group('test1')
	bld(features='cxx cprogram', source='main1.c', target='g1')

	bld.set_group('test2')
	obj2 = bld(features='cxx cprogram', source='main2.c', target='g2')

	bld.set_group('test4')
	obj2.clone('debug')
---------------

WARNING: By preventing parallelization, build groups may increase build times. On the other hand, they make projects more structured and improve the maintainance.


=== Build order constraints

==== set_run_after

The method _set_run_after_ is used to declare ordering constraints between tasks:

[source,python]
---------------
task1.set_run_after(task2)
---------------

The tasks are stored in the attribute _run_after_

==== computed constraints

===== after/before

The attributes _before_ and _after_ are used to declare ordering constraints between tasks:

[source,python]
---------------
from waflib import Task
class task_test_a(Task.TaskBase):
	before = 'task_test_b'
class task_test_b(Task.TaskBase):
	after = 'task_test_a'
---------------

===== ext_in/ext_out

Another way to declare precedence constraints is to declare a file extension production, for example:

[source,python]
---------------
from waflib import Task
class task_test_a(Task.TaskBase):
	ext_in = '.c'
class task_test_b(Task.TaskBase):
	ext_out = '.c'
---------------

The 'extensions' ext_in and ext_out have to match to add a valid precedence constraint, but they are only symbols in this context. They do not mean the tasks actually have to produce files of that type.

===== files inputs/outputs

Before feeding the tasks to the producer-consumer system, a constraint extraction is performed on the tasks having input and output files. The attributes _run_after_ are initialized with the tasks to wait for.

==== soft order constraints

Tasks that are known to take a lot of time may be launched first to improve the build times. The general problem of finding an optimal order for launching tasks in parallel and with constraints is called http://en.wikipedia.org/wiki/Job-shop_problem[Job Shop]. In practice this problem can often be reduced to a critical path problem (approximation).

The following pictures illustrate the difference in scheduling a build with different independent tasks, in which a slow task is clearly identified, and launched first:

image::duration-1{PIC}["Random order"{backend@docbook:,width=310:},align="center"]
image::duration-2{PIC}["Slowest task first"{backend@docbook:,width=310:},align="center"]

Waf provides a function for reordering the tasks before they are launched in the module Runner, the default reordering may be changed by dynamic method replacement in Python:

[source,python]
---------------
from waflib import Runner
def get_next(self):
	# reorder the task list by a random function
	self.outstanding.sort()
	# return the next task
	return self.outstanding.pop()
Runner.Parallel.get_next = get_next
---------------

If the reordering is not to be performed each time a task is retrieved, the list of task may be reordered when the next group is retrieved:

[source,python]
---------------
from waflib import Runner
old_refill = Runner.Parallel.refill_task_list
def refill_task_list(self):
	old_refill(self)
	self.outstanding.sort()
Runner.Parallel.refill_task_list = refill_task_list
---------------

It is possible to measure the task execution times by intercepting the function calls. The task execution times may be re-used for optimizing the schedule for subsequent builds:

[source,python]
---------------
import time
from waflib import Task
old_call_run = Task.TaskBase.call_run
def new_call_run(self):
	t1 = time.time()
	ret = old_call_run(self)
	t2 = time.time()
	if not ret: print("execution time %r" % (t2 - t1))
	return ret
Task.TaskBase.call_run = new_call_run
---------------


=== Dependencies

==== Task signatures

The direct instances of TaskBase are quite limited and do not track the changes to the source files. The class 'Task' provides the necessary features for the most common builds in which source files are used to produce target files. The idea is to create a unique signature for tasks, and to represent the dependencies on files or other tasks by including them in the signature. A hashing function is used for computing the signature, by default it is md5.

The following diagram illustrates the task processing including the signature, it is only valid for Task instance (not TaskBase instances):

image::task_signature{PIC}["Signatures"{backend@docbook:,width=320:},align="center"]

The signature computation uses the following data:

. explicit dependencies: input files and dependencies set explicitly using task.deps_man or bld.depends_on
. implicit dependencies: dependencies searched by the task itself (like source files included from other source files).
. parameters: compilation flags and command-line parameters.

Here is an example illustrating the different kinds of dependencies:

[source,python]
---------------
from waflib import Task
class task_demo(Task.Task):
	vars = ['CXXFLAGS', 'LINKFLAGS'] <1>
	def scan(self): <2>
		return [[self.inputs[0].parent.find_resource('.svn/entries')], []]

task = task_demo()
task.inputs = [bld.path.find_resource('test.cxx')] <3>
task.deps_man = [bld.path.find_resource('wscript')] <4>

bld.add_manual_dependency('main.c', 'an arbitrary string value') <5>
bld.add_manual_dependency(
		bld.path.find_or_declare('test_c_program'),
		bld.path.find_resource('bbb')) <6>
---------------

<1> Environment variable dependencies (compilation flags)
<2> Implicit dependencies: a method returns a list containing the list of additional nodes to take into account, and the list of the files that could not be found (cache)
<3> Explicit dependencies as input files (nodes)
<4> Explicit dependencies as manual dependencies
<5> Manual dependencies on source files, the second parameter can be a string, a node object or a function returning a string
<6> Manual dependencies with nodes, the first node represents a target (which may or may not exist in the build), and the second parameter represents a file in the source directory.

==== Explicit dependencies

TODO

==== Implicit dependencies and scanners

TODO

==== Variables

TODO


////
=== Task execution order

Running tasks in parallel is a simple problem, but in practice it is more complicated:
. Dependencies can be discovered during the build (dynamic task creation)
. New ordering constraints can be discovered after files are compiled
. The amount of tasks and ordering constraints (graph size) can be huge and performance may be a problem

To make the problem more simple, it is divided by the different concerns, and the ordering constraints can be given on three different levels:

. groups of tasks may run only after another group of tasks has finished to run, this represents a strict sequential order between groups of tasks, for example a compiler is produced and used to compile the tasks in the next group
. task types to indicate the instance will run after other task type instances, for example linking object files may only occur after compiling the source files
. specific constraints for task instances that can only run after a few other task instances

unlike the previous constraints, it is used on the instances of class 'Task' which is a subclass of class 'TaskBase'
////


